# ARK95X Configuration File
# Copy this to .env and configure your settings

# ═══════════════════════════════════════════════════════════════
# INSTALLATION PATHS
# ═══════════════════════════════════════════════════════════════
ARK95X_BASE_DIR="${HOME}/ark95x-complete"
ARK95X_PROD_DIR="${HOME}/ark95x-production"

# ═══════════════════════════════════════════════════════════════
# OLLAMA CONFIGURATION
# ═══════════════════════════════════════════════════════════════
OLLAMA_HOST="http://localhost:11434"
OLLAMA_NUM_CTX=4096  # Context window size
OLLAMA_NUM_THREAD=8  # CPU threads to use

# Models to install (comma-separated)
OLLAMA_MODELS="llama3.2:3b,deepseek-r1:7b,qwen2.5-coder:7b"

# ═══════════════════════════════════════════════════════════════
# API SERVER CONFIGURATION
# ═══════════════════════════════════════════════════════════════
API_HOST="0.0.0.0"
API_PORT=8000
API_WORKERS=4
API_RELOAD=false  # Set to true for development

# ═══════════════════════════════════════════════════════════════
# CLOUD AI API KEYS
# ═══════════════════════════════════════════════════════════════
# OpenAI
OPENAI_API_KEY="your-openai-key-here"

# Anthropic
ANTHROPIC_API_KEY="your-anthropic-key-here"

# Grok (xAI)
GROK_API_KEY="your-grok-key-here"

# Perplexity
PERPLEXITY_API_KEY="your-perplexity-key-here"

# ═══════════════════════════════════════════════════════════════
# ROUTING CONFIGURATION
# ═══════════════════════════════════════════════════════════════
# Default routing behavior
DEFAULT_FORCE_LOCAL=false  # Set to true to prefer local models
DEFAULT_TASK_TYPE="general"

# Task routing preferences
ROUTE_CODE_PRIMARY="qwen2.5-coder:7b"
ROUTE_CODE_FALLBACK="openai"

ROUTE_REASONING_PRIMARY="deepseek-r1:7b"
ROUTE_REASONING_FALLBACK="anthropic"

ROUTE_GENERAL_PRIMARY="llama3.2:3b"
ROUTE_GENERAL_FALLBACK="grok"

ROUTE_RESEARCH_PRIMARY="perplexity"
ROUTE_RESEARCH_FALLBACK="llama3.2:3b"

# ═══════════════════════════════════════════════════════════════
# MONITORING CONFIGURATION
# ═══════════════════════════════════════════════════════════════
HEALTH_CHECK_INTERVAL=60  # seconds
LOG_LEVEL="INFO"  # DEBUG, INFO, WARNING, ERROR
LOG_ROTATION_DAYS=7  # Keep logs for 7 days

# ═══════════════════════════════════════════════════════════════
# SECURITY CONFIGURATION
# ═══════════════════════════════════════════════════════════════
# API key encryption (upgrade to AES for production)
ENCRYPTION_METHOD="base64"  # Options: base64, aes256

# Rate limiting
RATE_LIMIT_ENABLED=true
RATE_LIMIT_REQUESTS=100  # requests per minute
RATE_LIMIT_WINDOW=60     # seconds

# ═══════════════════════════════════════════════════════════════
# DOCKER CONFIGURATION
# ═══════════════════════════════════════════════════════════════
DOCKER_OLLAMA_IMAGE="ollama/ollama:latest"
DOCKER_PYTHON_IMAGE="python:3.11-slim"

# ═══════════════════════════════════════════════════════════════
# PERFORMANCE TUNING
# ═══════════════════════════════════════════════════════════════
# Enable GPU acceleration (requires NVIDIA GPU)
OLLAMA_GPU_ENABLED=false
CUDA_VISIBLE_DEVICES="0"

# Memory limits
OLLAMA_MAX_MEMORY="8GB"
API_MAX_MEMORY="4GB"

# Request timeouts (seconds)
LOCAL_QUERY_TIMEOUT=60
CLOUD_QUERY_TIMEOUT=30

# ═══════════════════════════════════════════════════════════════
# BACKUP CONFIGURATION
# ═══════════════════════════════════════════════════════════════
BACKUP_ENABLED=true
BACKUP_DIR="${ARK95X_BASE_DIR}/backups"
BACKUP_INTERVAL_DAYS=1
BACKUP_RETENTION_DAYS=30

# ═══════════════════════════════════════════════════════════════
# EXPERIMENTAL FEATURES
# ═══════════════════════════════════════════════════════════════
ENABLE_WEBSOCKETS=false
ENABLE_STREAMING=false
ENABLE_MODEL_CACHING=true
