#!/bin/bash
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ARK95X ULTIMATE DEPLOYMENT - ONE COMMAND TO RULE THEM ALL
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#
# This script deploys:
# âœ… Ollama Local AI
# âœ… Multiple AI Models (optimized selection)
# âœ… API Key Management System
# âœ… Partisan Intelligence Multi-Layer System
# âœ… Internal Automation
# âœ… Health Monitoring
# âœ… Auto-start Scripts
# âœ… Complete Documentation
#
# Usage: ./ark95x_ultimate_deploy.sh
#
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

set -e  # Exit on error

# Colors
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
CYAN='\033[0;36m'
MAGENTA='\033[0;35m'
WHITE='\033[1;37m'
NC='\033[0;0m'

# Configuration
BASE_DIR="$HOME/ark95x-complete"
PROD_DIR="$HOME/ark95x-production"
REPO_URL="https://github.com/Ark95x-sAn/openai-assistants-quickstart.git"
BRANCH="claude/partisan-intelligence-01SVpXFVayXkCFEruDcyYoSZ"

# Logging
LOG_DIR="$BASE_DIR/logs"
LOG_FILE="$LOG_DIR/deployment_$(date +%Y%m%d_%H%M%S).log"
mkdir -p "$LOG_DIR"

# Log function
log() {
    echo -e "$1" | tee -a "$LOG_FILE"
}

# Progress tracking
TOTAL_STEPS=15
CURRENT_STEP=0

step() {
    CURRENT_STEP=$((CURRENT_STEP + 1))
    PERCENT=$((CURRENT_STEP * 100 / TOTAL_STEPS))
    log "${CYAN}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${NC}"
    log "${WHITE}[${CURRENT_STEP}/${TOTAL_STEPS}] ${PERCENT}% - $1${NC}"
    log "${CYAN}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${NC}"
}

# Banner
clear
log ""
log "${MAGENTA}â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—${NC}"
log "${MAGENTA}â•‘                                                           â•‘${NC}"
log "${MAGENTA}â•‘            ðŸ”¥ ARK95X ULTIMATE DEPLOYMENT ðŸ”¥              â•‘${NC}"
log "${MAGENTA}â•‘                                                           â•‘${NC}"
log "${MAGENTA}â•‘         Sovereign AI Intelligence System                  â•‘${NC}"
log "${MAGENTA}â•‘         Complete Local + Cloud Integration                â•‘${NC}"
log "${MAGENTA}â•‘                                                           â•‘${NC}"
log "${MAGENTA}â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${NC}"
log ""
log "${YELLOW}Started: $(date)${NC}"
log "${YELLOW}Log: $LOG_FILE${NC}"
log ""
sleep 2

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# STEP 1: CREATE DIRECTORY STRUCTURE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

step "Creating Directory Structure"

mkdir -p "$BASE_DIR"/{logs,scripts,configs,data,backups}
mkdir -p "$PROD_DIR"/{backend,frontend,configs,data/logs}

log "${GREEN}âœ“${NC} Base directory: $BASE_DIR"
log "${GREEN}âœ“${NC} Production directory: $PROD_DIR"

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# STEP 2: SYSTEM PREREQUISITES CHECK
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

step "Checking System Prerequisites"

# Check Python
if command -v python3 &> /dev/null; then
    PYTHON_VERSION=$(python3 --version | grep -oP '\d+\.\d+')
    log "${GREEN}âœ“${NC} Python $PYTHON_VERSION installed"
else
    log "${RED}âœ—${NC} Python3 not found - Installing..."
    sudo apt-get update && sudo apt-get install -y python3 python3-pip
fi

# Check Docker
if command -v docker &> /dev/null; then
    log "${GREEN}âœ“${NC} Docker installed"
else
    log "${YELLOW}âš ${NC} Docker not found - Installing..."
    curl -fsSL https://get.docker.com | sh
    sudo usermod -aG docker $USER
    log "${GREEN}âœ“${NC} Docker installed (may need logout/login)"
fi

# Check Docker Compose
if command -v docker-compose &> /dev/null; then
    log "${GREEN}âœ“${NC} Docker Compose installed"
else
    log "${YELLOW}âš ${NC} Installing Docker Compose..."
    sudo apt-get install -y docker-compose
fi

# Check Git
if command -v git &> /dev/null; then
    log "${GREEN}âœ“${NC} Git installed"
else
    log "${YELLOW}âš ${NC} Installing Git..."
    sudo apt-get install -y git
fi

# Install Python packages
log "${BLUE}Installing Python dependencies...${NC}"
pip3 install -q fastapi uvicorn requests pydantic aiofiles pyyaml motor pymongo redis websockets 2>&1 | tee -a "$LOG_FILE"

log "${GREEN}âœ“${NC} All prerequisites satisfied"

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# STEP 3: INSTALL AND START OLLAMA
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

step "Installing Ollama Local AI"

if ! command -v ollama &> /dev/null; then
    log "${BLUE}Downloading and installing Ollama...${NC}"
    curl -fsSL https://ollama.com/install.sh | sh
    log "${GREEN}âœ“${NC} Ollama installed"
else
    log "${GREEN}âœ“${NC} Ollama already installed"
fi

# Start Ollama service
log "${BLUE}Starting Ollama service...${NC}"
if ! pgrep -x ollama > /dev/null; then
    nohup ollama serve > "$LOG_DIR/ollama.log" 2>&1 &
    sleep 5
    log "${GREEN}âœ“${NC} Ollama service started"
else
    log "${GREEN}âœ“${NC} Ollama already running"
fi

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# STEP 4: PULL OPTIMIZED AI MODELS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

step "Pulling Optimized AI Models"

log "${BLUE}This will download ~4-6GB of models (optimized selection)${NC}"
log "${BLUE}Progress will be shown below...${NC}"
log ""

# Model selection optimized for performance/size balance
declare -a MODELS=(
    "llama3.2:3b"
    "deepseek-r1:7b"
    "qwen2.5-coder:7b"
)

declare -A MODEL_DESC=(
    ["llama3.2:3b"]="Fast general purpose (2GB)"
    ["deepseek-r1:7b"]="Advanced reasoning (4GB)"
    ["qwen2.5-coder:7b"]="Code specialist (4GB)"
)

for model in "${MODELS[@]}"; do
    log "${YELLOW}â†’${NC} Pulling $model - ${MODEL_DESC[$model]}"

    if ollama list | grep -q "^${model}"; then
        log "${GREEN}âœ“${NC} $model already installed"
    else
        ollama pull "$model" 2>&1 | tee -a "$LOG_FILE"
        log "${GREEN}âœ“${NC} $model ready"
    fi
done

log ""
log "${GREEN}âœ“${NC} All models installed"

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# STEP 5: SETUP API KEY MANAGEMENT
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

step "Setting Up API Key Management"

# Create API Key Manager
cat > "$PROD_DIR/backend/api_key_manager.py" << 'EOPYTHON'
#!/usr/bin/env python3
"""ARK95X API Key Manager - Production Ready"""

import json
import base64
import hashlib
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Optional

class APIKeyManager:
    def __init__(self):
        self.config_path = Path.home() / "ark95x-production" / "configs" / "api_keys.json"
        self.config_path.parent.mkdir(parents=True, exist_ok=True)
        self.keys = {}
        self.current_index = {}
        self.usage_stats = {}
        self.load_keys()

    def encrypt(self, data: str) -> str:
        """Simple encryption for storage"""
        return base64.b64encode(data.encode()).decode()

    def decrypt(self, data: str) -> str:
        """Decrypt stored keys"""
        return base64.b64decode(data.encode()).decode()

    def add_key(self, provider: str, api_key: str, name: str = None):
        """Add a new API key"""
        key_id = hashlib.md5(f"{provider}:{api_key}".encode()).hexdigest()[:8]

        if provider not in self.keys:
            self.keys[provider] = []
            self.current_index[provider] = 0

        self.keys[provider].append({
            "id": key_id,
            "name": name or f"{provider}_{key_id}",
            "provider": provider,
            "key": self.encrypt(api_key),
            "status": "active",
            "added_at": datetime.now().isoformat(),
            "success_count": 0,
            "failure_count": 0
        })

        self.usage_stats[key_id] = {"requests": 0, "successes": 0, "failures": 0}
        self.save_keys()
        return key_id

    def get_next_key(self, provider: str) -> Optional[Dict]:
        """Get next available key for provider (round-robin)"""
        if provider not in self.keys or not self.keys[provider]:
            return None

        active_keys = [k for k in self.keys[provider] if k["status"] == "active"]
        if not active_keys:
            return None

        idx = self.current_index[provider] % len(active_keys)
        key_data = active_keys[idx]

        # Rotate to next key
        self.current_index[provider] = (idx + 1) % len(active_keys)

        decrypted_key = self.decrypt(key_data["key"])
        self.usage_stats[key_data["id"]]["requests"] += 1
        self.save_keys()

        return {
            "id": key_data["id"],
            "name": key_data["name"],
            "key": decrypted_key
        }

    def mark_success(self, key_id: str):
        """Mark a key as successful"""
        if key_id in self.usage_stats:
            self.usage_stats[key_id]["successes"] += 1
            self.save_keys()

    def mark_failure(self, key_id: str):
        """Mark a key as failed"""
        if key_id in self.usage_stats:
            self.usage_stats[key_id]["failures"] += 1
            self.save_keys()

    def get_stats(self) -> Dict:
        """Get usage statistics"""
        return {
            "total_keys": sum(len(keys) for keys in self.keys.values()),
            "total_requests": sum(s["requests"] for s in self.usage_stats.values()),
            "by_provider": {
                p: {
                    "total": len(k),
                    "active": sum(1 for x in k if x["status"]=="active")
                }
                for p, k in self.keys.items()
            }
        }

    def save_keys(self):
        """Save keys to file"""
        with open(self.config_path, 'w') as f:
            json.dump({
                "keys": self.keys,
                "current_index": self.current_index,
                "usage_stats": self.usage_stats
            }, f, indent=2)

    def load_keys(self):
        """Load keys from file"""
        if self.config_path.exists():
            with open(self.config_path, 'r') as f:
                data = json.load(f)
                self.keys = data.get("keys", {})
                self.current_index = data.get("current_index", {})
                self.usage_stats = data.get("usage_stats", {})

# Global instance
key_manager = APIKeyManager()

if __name__ == "__main__":
    print("API Key Manager initialized")
    print(f"Stats: {key_manager.get_stats()}")
EOPYTHON

# Create key setup script
cat > "$BASE_DIR/scripts/setup_api_keys.py" << 'EOKEYS'
#!/usr/bin/env python3
"""Setup API Keys for ARK95X System"""

import sys
from pathlib import Path
sys.path.insert(0, str(Path.home() / "ark95x-production" / "backend"))
from api_key_manager import key_manager

# Add your API keys here - REPLACE WITH YOUR ACTUAL KEYS
# Get keys from:
# - OpenAI: https://platform.openai.com/api-keys
# - Anthropic: https://console.anthropic.com/
# - Grok: https://console.x.ai/
# - Perplexity: https://www.perplexity.ai/settings/api

keys_to_add = {
    "openai": ("YOUR_OPENAI_API_KEY_HERE", "OpenAI GPT-4"),
    "anthropic": ("YOUR_ANTHROPIC_API_KEY_HERE", "Anthropic Claude"),
    "grok": ("YOUR_GROK_API_KEY_HERE", "Grok xAI"),
    "perplexity": ("YOUR_PERPLEXITY_API_KEY_HERE", "Perplexity AI")
}

# Alternative: Use environment variables (recommended for production)
# import os
# keys_to_add = {
#     "openai": (os.getenv("OPENAI_API_KEY"), "OpenAI GPT-4"),
#     "anthropic": (os.getenv("ANTHROPIC_API_KEY"), "Anthropic Claude"),
#     "grok": (os.getenv("GROK_API_KEY"), "Grok xAI"),
#     "perplexity": (os.getenv("PERPLEXITY_API_KEY"), "Perplexity AI")
# }

def main():
    print("ðŸ”‘ Setting up API keys...")
    for provider, (key, name) in keys_to_add.items():
        key_id = key_manager.add_key(provider, key, name)
        print(f"  âœ“ {name} (ID: {key_id})")

    print(f"\nâœ… {len(keys_to_add)} API keys configured")
    stats = key_manager.get_stats()
    print(f"ðŸ“Š Total keys: {stats['total_keys']}")
    print(f"ðŸ“Š Total requests: {stats['total_requests']}")

    for provider, info in stats['by_provider'].items():
        print(f"   {provider}: {info['active']}/{info['total']} active")

if __name__ == "__main__":
    main()
EOKEYS

chmod +x "$BASE_DIR/scripts/setup_api_keys.py"

# Run key setup
log "${BLUE}Configuring API keys...${NC}"
python3 "$BASE_DIR/scripts/setup_api_keys.py" 2>&1 | tee -a "$LOG_FILE"

log "${GREEN}âœ“${NC} API Key Management configured"

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# STEP 6: CLONE PARTISAN INTELLIGENCE REPOSITORY
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

step "Setting Up Partisan Intelligence System"

if [ -d "$BASE_DIR/partisan-intelligence" ]; then
    log "${YELLOW}âš ${NC} Partisan Intelligence already cloned, pulling updates..."
    cd "$BASE_DIR/partisan-intelligence"
    git pull origin "$BRANCH" 2>&1 | tee -a "$LOG_FILE"
else
    log "${BLUE}Cloning Partisan Intelligence repository...${NC}"
    cd "$BASE_DIR"
    git clone "$REPO_URL" partisan-intelligence 2>&1 | tee -a "$LOG_FILE"
    cd partisan-intelligence
    git checkout "$BRANCH" 2>&1 | tee -a "$LOG_FILE"
fi

log "${GREEN}âœ“${NC} Partisan Intelligence repository ready"

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# STEP 7: CREATE UNIFIED API GATEWAY
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

step "Creating Unified API Gateway"

cat > "$PROD_DIR/backend/unified_api_gateway.py" << 'EOGATEWAY'
#!/usr/bin/env python3
"""ARK95X Unified API Gateway - Routes to best available AI"""

import asyncio
import sys
from pathlib import Path
from typing import Dict, Optional
import requests
from datetime import datetime

# Add backend to path
sys.path.insert(0, str(Path(__file__).parent))
from api_key_manager import key_manager

class UnifiedAIGateway:
    def __init__(self):
        self.ollama_base = "http://localhost:11434"
        self.providers = {
            "openai": "https://api.openai.com/v1/chat/completions",
            "anthropic": "https://api.anthropic.com/v1/messages",
            "grok": "https://api.x.ai/v1/chat/completions",
            "perplexity": "https://api.perplexity.ai/chat/completions"
        }

    def query_ollama(self, model: str, prompt: str, stream: bool = False) -> Dict:
        """Query local Ollama instance"""
        try:
            response = requests.post(
                f"{self.ollama_base}/api/generate",
                json={"model": model, "prompt": prompt, "stream": stream},
                timeout=30
            )
            response.raise_for_status()
            return {"success": True, "response": response.json()["response"], "source": "ollama"}
        except Exception as e:
            return {"success": False, "error": str(e), "source": "ollama"}

    def query_openai(self, prompt: str, model: str = "gpt-4") -> Dict:
        """Query OpenAI API"""
        key_data = key_manager.get_next_key("openai")
        if not key_data:
            return {"success": False, "error": "No OpenAI keys available"}

        try:
            response = requests.post(
                self.providers["openai"],
                headers={
                    "Authorization": f"Bearer {key_data['key']}",
                    "Content-Type": "application/json"
                },
                json={
                    "model": model,
                    "messages": [{"role": "user", "content": prompt}]
                },
                timeout=30
            )
            response.raise_for_status()
            key_manager.mark_success(key_data['id'])
            return {
                "success": True,
                "response": response.json()["choices"][0]["message"]["content"],
                "source": "openai"
            }
        except Exception as e:
            key_manager.mark_failure(key_data['id'])
            return {"success": False, "error": str(e), "source": "openai"}

    def query_anthropic(self, prompt: str, model: str = "claude-3-5-sonnet-20241022") -> Dict:
        """Query Anthropic API"""
        key_data = key_manager.get_next_key("anthropic")
        if not key_data:
            return {"success": False, "error": "No Anthropic keys available"}

        try:
            response = requests.post(
                self.providers["anthropic"],
                headers={
                    "x-api-key": key_data['key'],
                    "anthropic-version": "2023-06-01",
                    "Content-Type": "application/json"
                },
                json={
                    "model": model,
                    "max_tokens": 4096,
                    "messages": [{"role": "user", "content": prompt}]
                },
                timeout=30
            )
            response.raise_for_status()
            key_manager.mark_success(key_data['id'])
            return {
                "success": True,
                "response": response.json()["content"][0]["text"],
                "source": "anthropic"
            }
        except Exception as e:
            key_manager.mark_failure(key_data['id'])
            return {"success": False, "error": str(e), "source": "anthropic"}

    def intelligent_route(self, prompt: str, preference: str = "local") -> Dict:
        """Intelligently route query to best available AI"""

        if preference == "local":
            # Try Ollama first
            result = self.query_ollama("llama3.2:3b", prompt)
            if result["success"]:
                return result

        # Try cloud providers in order
        providers_order = ["anthropic", "openai", "grok", "perplexity"]

        for provider in providers_order:
            if provider == "anthropic":
                result = self.query_anthropic(prompt)
            elif provider == "openai":
                result = self.query_openai(prompt)

            if result["success"]:
                return result

        # Last resort: try Ollama even if preference was cloud
        if preference != "local":
            return self.query_ollama("llama3.2:3b", prompt)

        return {"success": False, "error": "All providers failed"}

# Global gateway instance
gateway = UnifiedAIGateway()

if __name__ == "__main__":
    print("Testing Unified AI Gateway...")
    result = gateway.intelligent_route("Say hello in one sentence.", preference="local")
    print(f"\nResult: {result}")
EOGATEWAY

chmod +x "$PROD_DIR/backend/unified_api_gateway.py"

log "${GREEN}âœ“${NC} Unified API Gateway created"

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# STEP 8: CREATE HEALTH MONITORING SYSTEM
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

step "Creating Health Monitoring System"

cat > "$PROD_DIR/backend/health_monitor.py" << 'EOHEALTH'
#!/usr/bin/env python3
"""ARK95X Health Monitoring System"""

import requests
import subprocess
import sys
from pathlib import Path
from datetime import datetime
from typing import Dict

sys.path.insert(0, str(Path(__file__).parent))
from api_key_manager import key_manager

class HealthMonitor:
    def __init__(self):
        self.ollama_base = "http://localhost:11434"
        self.status = {}

    def check_ollama(self) -> Dict:
        """Check if Ollama is running"""
        try:
            response = requests.get(f"{self.ollama_base}/api/tags", timeout=5)
            models = response.json().get("models", [])
            return {
                "status": "healthy",
                "models_count": len(models),
                "models": [m["name"] for m in models]
            }
        except Exception as e:
            return {"status": "unhealthy", "error": str(e)}

    def check_api_keys(self) -> Dict:
        """Check API key status"""
        stats = key_manager.get_stats()
        return {
            "status": "healthy" if stats["total_keys"] > 0 else "warning",
            "total_keys": stats["total_keys"],
            "total_requests": stats["total_requests"],
            "by_provider": stats["by_provider"]
        }

    def check_system_resources(self) -> Dict:
        """Check system resources"""
        try:
            # Check disk space
            disk = subprocess.check_output(["df", "-h", "/"], text=True).split("\n")[1].split()
            disk_usage = disk[4].rstrip("%")

            # Check memory
            mem = subprocess.check_output(["free", "-h"], text=True).split("\n")[1].split()

            return {
                "status": "healthy" if int(disk_usage) < 90 else "warning",
                "disk_usage": f"{disk_usage}%",
                "disk_available": disk[3],
                "memory_available": mem[6]
            }
        except Exception as e:
            return {"status": "unknown", "error": str(e)}

    def run_health_check(self) -> Dict:
        """Run complete health check"""
        self.status = {
            "timestamp": datetime.now().isoformat(),
            "ollama": self.check_ollama(),
            "api_keys": self.check_api_keys(),
            "system": self.check_system_resources()
        }

        # Determine overall status
        statuses = [
            self.status["ollama"]["status"],
            self.status["api_keys"]["status"],
            self.status["system"]["status"]
        ]

        if "unhealthy" in statuses:
            self.status["overall"] = "unhealthy"
        elif "warning" in statuses:
            self.status["overall"] = "warning"
        else:
            self.status["overall"] = "healthy"

        return self.status

    def print_status(self):
        """Print formatted status"""
        status = self.run_health_check()

        symbols = {"healthy": "âœ“", "warning": "âš ", "unhealthy": "âœ—", "unknown": "?"}
        colors = {
            "healthy": "\033[0;32m",
            "warning": "\033[1;33m",
            "unhealthy": "\033[0;31m",
            "unknown": "\033[0;37m"
        }
        NC = "\033[0m"

        overall = status["overall"]
        color = colors.get(overall, NC)
        symbol = symbols.get(overall, "?")

        print(f"\n{color}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•{NC}")
        print(f"{color}  ARK95X SYSTEM HEALTH CHECK{NC}")
        print(f"{color}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•{NC}\n")

        print(f"Overall Status: {color}{symbol} {overall.upper()}{NC}")
        print(f"Timestamp: {status['timestamp']}\n")

        # Ollama status
        ollama = status["ollama"]
        ollama_symbol = symbols.get(ollama["status"], "?")
        ollama_color = colors.get(ollama["status"], NC)
        print(f"{ollama_color}{ollama_symbol}{NC} Ollama: {ollama['status']}")
        if "models_count" in ollama:
            print(f"  Models: {ollama['models_count']} installed")

        # API Keys status
        api = status["api_keys"]
        api_symbol = symbols.get(api["status"], "?")
        api_color = colors.get(api["status"], NC)
        print(f"{api_color}{api_symbol}{NC} API Keys: {api['status']}")
        print(f"  Total: {api['total_keys']} keys, {api['total_requests']} requests")

        # System status
        system = status["system"]
        sys_symbol = symbols.get(system["status"], "?")
        sys_color = colors.get(system["status"], NC)
        print(f"{sys_color}{sys_symbol}{NC} System: {system['status']}")
        if "disk_usage" in system:
            print(f"  Disk: {system['disk_usage']} used, {system['disk_available']} available")

        print(f"\n{color}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•{NC}\n")

# Create instance
monitor = HealthMonitor()

if __name__ == "__main__":
    monitor.print_status()
EOHEALTH

chmod +x "$PROD_DIR/backend/health_monitor.py"

log "${GREEN}âœ“${NC} Health Monitoring System created"

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# STEP 9: CREATE AUTO-START SERVICES
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

step "Creating Auto-Start Services"

# Create systemd service for Ollama
cat > "$BASE_DIR/configs/ollama.service" << 'EOSERVICE'
[Unit]
Description=Ollama AI Service
After=network.target

[Service]
Type=simple
ExecStart=/usr/local/bin/ollama serve
Restart=always
RestartSec=10
User=%u

[Install]
WantedBy=multi-user.target
EOSERVICE

# Create startup script
cat > "$BASE_DIR/scripts/start_ark95x.sh" << 'EOSTARTUP'
#!/bin/bash
# ARK95X System Startup Script

set -e

BASE_DIR="$HOME/ark95x-complete"
PROD_DIR="$HOME/ark95x-production"
LOG_DIR="$BASE_DIR/logs"

echo "ðŸš€ Starting ARK95X System..."

# Start Ollama if not running
if ! pgrep -x ollama > /dev/null; then
    echo "  â†’ Starting Ollama service..."
    nohup ollama serve > "$LOG_DIR/ollama.log" 2>&1 &
    sleep 3
    echo "  âœ“ Ollama started"
else
    echo "  âœ“ Ollama already running"
fi

# Run health check
echo ""
echo "Running health check..."
python3 "$PROD_DIR/backend/health_monitor.py"

echo ""
echo "âœ… ARK95X System is ready!"
echo ""
echo "Available commands:"
echo "  - Health check: python3 $PROD_DIR/backend/health_monitor.py"
echo "  - API gateway test: python3 $PROD_DIR/backend/unified_api_gateway.py"
echo "  - Ollama CLI: ollama run llama3.2:3b"
echo ""
EOSTARTUP

chmod +x "$BASE_DIR/scripts/start_ark95x.sh"

# Create stop script
cat > "$BASE_DIR/scripts/stop_ark95x.sh" << 'EOSTOP'
#!/bin/bash
# ARK95X System Stop Script

echo "ðŸ›‘ Stopping ARK95X System..."

# Stop Ollama
if pgrep -x ollama > /dev/null; then
    echo "  â†’ Stopping Ollama..."
    pkill ollama
    echo "  âœ“ Ollama stopped"
else
    echo "  âœ“ Ollama not running"
fi

echo "âœ… ARK95X System stopped"
EOSTOP

chmod +x "$BASE_DIR/scripts/stop_ark95x.sh"

log "${GREEN}âœ“${NC} Auto-start scripts created"

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# STEP 10: CREATE CONFIGURATION TEMPLATES
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

step "Creating Configuration Templates"

# Create environment template
cat > "$BASE_DIR/configs/environment.template" << 'EOENV'
# ARK95X Environment Configuration

# Directories
ARK95X_BASE_DIR="$HOME/ark95x-complete"
ARK95X_PROD_DIR="$HOME/ark95x-production"
ARK95X_LOG_DIR="$HOME/ark95x-complete/logs"

# Ollama Configuration
OLLAMA_HOST="http://localhost:11434"
OLLAMA_MODELS="llama3.2:3b,deepseek-r1:7b,qwen2.5-coder:7b"

# API Configuration
API_KEY_CONFIG="$HOME/ark95x-production/configs/api_keys.json"

# Logging
LOG_LEVEL="INFO"
LOG_RETENTION_DAYS=30

# Feature Flags
ENABLE_OLLAMA=true
ENABLE_CLOUD_APIS=true
ENABLE_HEALTH_MONITORING=true
EOENV

# Create Docker Compose for advanced users
cat > "$BASE_DIR/configs/docker-compose.yml" << 'EODOCKER'
version: '3.8'

services:
  ollama:
    image: ollama/ollama:latest
    container_name: ark95x-ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  ark95x-api:
    build:
      context: ../production/backend
      dockerfile: Dockerfile
    container_name: ark95x-api
    ports:
      - "8000:8000"
    environment:
      - OLLAMA_HOST=http://ollama:11434
    depends_on:
      - ollama
    restart: unless-stopped

volumes:
  ollama-data:
EODOCKER

log "${GREEN}âœ“${NC} Configuration templates created"

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# STEP 11: CREATE BACKUP SYSTEM
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

step "Creating Backup System"

cat > "$BASE_DIR/scripts/backup_ark95x.sh" << 'EOBACKUP'
#!/bin/bash
# ARK95X Backup Script

set -e

BASE_DIR="$HOME/ark95x-complete"
PROD_DIR="$HOME/ark95x-production"
BACKUP_DIR="$BASE_DIR/backups"
TIMESTAMP=$(date +%Y%m%d_%H%M%S)
BACKUP_FILE="$BACKUP_DIR/ark95x_backup_$TIMESTAMP.tar.gz"

echo "ðŸ“¦ Creating ARK95X Backup..."

mkdir -p "$BACKUP_DIR"

# Create backup
tar -czf "$BACKUP_FILE" \
    -C "$HOME" \
    ark95x-production/configs \
    ark95x-production/backend \
    ark95x-complete/configs \
    ark95x-complete/scripts \
    2>/dev/null || true

# Get backup size
BACKUP_SIZE=$(du -h "$BACKUP_FILE" | cut -f1)

echo "âœ… Backup created: $BACKUP_FILE"
echo "   Size: $BACKUP_SIZE"

# Clean old backups (keep last 5)
cd "$BACKUP_DIR"
ls -t ark95x_backup_*.tar.gz | tail -n +6 | xargs -r rm

REMAINING=$(ls -1 ark95x_backup_*.tar.gz 2>/dev/null | wc -l)
echo "   Backups retained: $REMAINING"
EOBACKUP

chmod +x "$BASE_DIR/scripts/backup_ark95x.sh"

log "${GREEN}âœ“${NC} Backup system created"

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# STEP 12: CREATE USAGE EXAMPLES
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

step "Creating Usage Examples"

cat > "$BASE_DIR/scripts/example_usage.py" << 'EOEXAMPLE'
#!/usr/bin/env python3
"""ARK95X Usage Examples"""

import sys
from pathlib import Path

# Add backend to path
sys.path.insert(0, str(Path.home() / "ark95x-production" / "backend"))

from unified_api_gateway import gateway
from api_key_manager import key_manager
from health_monitor import monitor

def example_ollama_query():
    """Example: Query local Ollama model"""
    print("\n" + "="*50)
    print("Example 1: Local Ollama Query")
    print("="*50)

    result = gateway.query_ollama("llama3.2:3b", "Explain quantum computing in one sentence.")

    if result["success"]:
        print(f"âœ“ Success!")
        print(f"Response: {result['response'][:200]}...")
    else:
        print(f"âœ— Failed: {result['error']}")

def example_intelligent_routing():
    """Example: Intelligent routing (tries local first, then cloud)"""
    print("\n" + "="*50)
    print("Example 2: Intelligent Routing")
    print("="*50)

    result = gateway.intelligent_route(
        "What is the capital of France?",
        preference="local"
    )

    if result["success"]:
        print(f"âœ“ Success via {result['source']}!")
        print(f"Response: {result['response']}")
    else:
        print(f"âœ— Failed: {result['error']}")

def example_api_stats():
    """Example: Check API key statistics"""
    print("\n" + "="*50)
    print("Example 3: API Key Statistics")
    print("="*50)

    stats = key_manager.get_stats()
    print(f"Total API keys: {stats['total_keys']}")
    print(f"Total requests: {stats['total_requests']}")
    print("\nBy provider:")
    for provider, info in stats['by_provider'].items():
        print(f"  {provider}: {info['active']}/{info['total']} active")

def example_health_check():
    """Example: Run health check"""
    print("\n" + "="*50)
    print("Example 4: System Health Check")
    print("="*50)

    monitor.print_status()

if __name__ == "__main__":
    print("\nðŸš€ ARK95X Usage Examples\n")

    example_ollama_query()
    example_intelligent_routing()
    example_api_stats()
    example_health_check()

    print("\nâœ… Examples complete!\n")
EOEXAMPLE

chmod +x "$BASE_DIR/scripts/example_usage.py"

log "${GREEN}âœ“${NC} Usage examples created"

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# STEP 13: CREATE MAINTENANCE SCRIPTS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

step "Creating Maintenance Scripts"

cat > "$BASE_DIR/scripts/update_models.sh" << 'EOUPDATE'
#!/bin/bash
# Update all Ollama models

echo "ðŸ”„ Updating Ollama models..."

MODELS=("llama3.2:3b" "deepseek-r1:7b" "qwen2.5-coder:7b")

for model in "${MODELS[@]}"; do
    echo "  â†’ Updating $model..."
    ollama pull "$model"
    echo "  âœ“ $model updated"
done

echo "âœ… All models updated!"
EOUPDATE

chmod +x "$BASE_DIR/scripts/update_models.sh"

cat > "$BASE_DIR/scripts/clean_logs.sh" << 'EOCLEAN'
#!/bin/bash
# Clean old logs

BASE_DIR="$HOME/ark95x-complete"
LOG_DIR="$BASE_DIR/logs"
DAYS_TO_KEEP=30

echo "ðŸ§¹ Cleaning logs older than $DAYS_TO_KEEP days..."

find "$LOG_DIR" -name "*.log" -type f -mtime +$DAYS_TO_KEEP -delete

REMAINING=$(find "$LOG_DIR" -name "*.log" -type f | wc -l)
echo "âœ… Cleanup complete. $REMAINING log files remaining."
EOCLEAN

chmod +x "$BASE_DIR/scripts/clean_logs.sh"

log "${GREEN}âœ“${NC} Maintenance scripts created"

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# STEP 14: CREATE COMPREHENSIVE DOCUMENTATION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

step "Creating Documentation"

cat > "$BASE_DIR/README.md" << 'EODOC'
# ðŸ”¥ ARK95X Sovereign AI Intelligence System

## Overview

ARK95X is a complete AI orchestration system combining local (Ollama) and cloud-based AI models with intelligent routing, API key management, and comprehensive monitoring.

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           ARK95X Unified AI Gateway                 â”‚
â”‚                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Local Ollama  â”‚      â”‚   Cloud APIs          â”‚  â”‚
â”‚  â”‚ - LLama 3.2   â”‚â—„â”€â”€â”€â”€â–ºâ”‚ - OpenAI GPT-4       â”‚  â”‚
â”‚  â”‚ - DeepSeek R1 â”‚      â”‚ - Anthropic Claude   â”‚  â”‚
â”‚  â”‚ - Qwen Coder  â”‚      â”‚ - Grok xAI           â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚ - Perplexity AI      â”‚  â”‚
â”‚                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  API Key Manager (Round-robin, failover)      â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Health Monitor (System & AI status)          â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Quick Start

### 1. Start the System

```bash
~/ark95x-complete/scripts/start_ark95x.sh
```

### 2. Check Health

```bash
python3 ~/ark95x-production/backend/health_monitor.py
```

### 3. Test AI Gateway

```bash
python3 ~/ark95x-production/backend/unified_api_gateway.py
```

### 4. Run Examples

```bash
python3 ~/ark95x-complete/scripts/example_usage.py
```

## Components

### Local AI (Ollama)

**Models Installed:**
- **llama3.2:3b** - Fast general purpose (2GB)
- **deepseek-r1:7b** - Advanced reasoning (4GB)
- **qwen2.5-coder:7b** - Code specialist (4GB)

**Direct Usage:**
```bash
ollama run llama3.2:3b "Your question here"
ollama run deepseek-r1:7b "Complex reasoning task"
ollama run qwen2.5-coder:7b "Write a Python function"
```

### Cloud APIs

**Configured Providers:**
- OpenAI GPT-4
- Anthropic Claude
- Grok xAI
- Perplexity AI

**Intelligent Routing:**
The system automatically tries local models first, then falls back to cloud APIs if needed.

### API Key Management

**Features:**
- Round-robin key rotation
- Automatic failover
- Usage tracking
- Success/failure statistics

**Add New Keys:**
```python
from api_key_manager import key_manager
key_manager.add_key("provider", "your-api-key", "Description")
```

### Health Monitoring

**Monitors:**
- Ollama service status
- Available models
- API key pool status
- System resources (disk, memory)

**Run Health Check:**
```bash
python3 ~/ark95x-production/backend/health_monitor.py
```

## Directory Structure

```
~/ark95x-complete/
â”œâ”€â”€ logs/              # System logs
â”œâ”€â”€ scripts/           # Utility scripts
â”‚   â”œâ”€â”€ start_ark95x.sh
â”‚   â”œâ”€â”€ stop_ark95x.sh
â”‚   â”œâ”€â”€ backup_ark95x.sh
â”‚   â”œâ”€â”€ update_models.sh
â”‚   â””â”€â”€ example_usage.py
â”œâ”€â”€ configs/           # Configuration files
â”‚   â”œâ”€â”€ environment.template
â”‚   â”œâ”€â”€ ollama.service
â”‚   â””â”€â”€ docker-compose.yml
â”œâ”€â”€ data/             # Application data
â””â”€â”€ backups/          # System backups

~/ark95x-production/
â”œâ”€â”€ backend/          # Core Python modules
â”‚   â”œâ”€â”€ api_key_manager.py
â”‚   â”œâ”€â”€ unified_api_gateway.py
â”‚   â””â”€â”€ health_monitor.py
â”œâ”€â”€ configs/          # Runtime configs
â”‚   â””â”€â”€ api_keys.json
â””â”€â”€ data/logs/        # Application logs
```

## Usage Examples

### Example 1: Simple Query

```python
from unified_api_gateway import gateway

result = gateway.intelligent_route(
    "Explain machine learning in simple terms",
    preference="local"  # Try local first
)

print(result['response'])
```

### Example 2: Code Generation

```python
result = gateway.query_ollama(
    "qwen2.5-coder:7b",
    "Write a Python function to calculate fibonacci numbers"
)

print(result['response'])
```

### Example 3: Advanced Reasoning

```python
result = gateway.query_ollama(
    "deepseek-r1:7b",
    "Analyze the pros and cons of renewable energy"
)

print(result['response'])
```

## Maintenance

### Update Models

```bash
~/ark95x-complete/scripts/update_models.sh
```

### Backup System

```bash
~/ark95x-complete/scripts/backup_ark95x.sh
```

### Clean Old Logs

```bash
~/ark95x-complete/scripts/clean_logs.sh
```

## Troubleshooting

### Ollama Not Starting

```bash
# Check if Ollama is installed
which ollama

# Manual start
ollama serve

# Check logs
tail -f ~/ark95x-complete/logs/ollama.log
```

### Models Not Found

```bash
# List installed models
ollama list

# Pull missing model
ollama pull llama3.2:3b
```

### API Keys Not Working

```bash
# Check key configuration
python3 -c "from api_key_manager import key_manager; print(key_manager.get_stats())"

# Reconfigure keys
python3 ~/ark95x-complete/scripts/setup_api_keys.py
```

## Advanced Configuration

### Enable Systemd Service (Auto-start on boot)

```bash
sudo cp ~/ark95x-complete/configs/ollama.service /etc/systemd/system/
sudo systemctl enable ollama
sudo systemctl start ollama
```

### Docker Deployment

```bash
cd ~/ark95x-complete/configs
docker-compose up -d
```

## Performance Tips

1. **Local-first Strategy**: Use `preference="local"` for better latency and no API costs
2. **Model Selection**:
   - Use llama3.2:3b for quick queries
   - Use deepseek-r1:7b for complex reasoning
   - Use qwen2.5-coder:7b for code tasks
3. **Resource Management**: Monitor disk space and memory usage
4. **Key Rotation**: System automatically rotates keys for rate limit management

## Security Notes

- API keys are base64 encoded (not encrypted) - store config files securely
- Restrict access to `~/ark95x-production/configs/api_keys.json`
- Consider using environment variables for production deployments
- Enable firewall rules if exposing services

## Support

For issues or questions:
- Check logs in `~/ark95x-complete/logs/`
- Run health check to diagnose issues
- Review configuration in `~/ark95x-complete/configs/`

## License

Private deployment - All rights reserved.

---

**ARK95X** - Sovereign AI Intelligence System
*Local Control. Cloud Power. Infinite Possibilities.*
EODOC

log "${GREEN}âœ“${NC} Documentation created"

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# STEP 15: FINAL VALIDATION AND SUMMARY
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

step "Running Final Validation"

# Test Python scripts syntax
log "${BLUE}Validating Python scripts...${NC}"
python3 -m py_compile "$PROD_DIR/backend/api_key_manager.py"
python3 -m py_compile "$PROD_DIR/backend/unified_api_gateway.py"
python3 -m py_compile "$PROD_DIR/backend/health_monitor.py"
log "${GREEN}âœ“${NC} All Python scripts valid"

# Test bash scripts syntax
log "${BLUE}Validating shell scripts...${NC}"
bash -n "$BASE_DIR/scripts/start_ark95x.sh"
bash -n "$BASE_DIR/scripts/stop_ark95x.sh"
bash -n "$BASE_DIR/scripts/backup_ark95x.sh"
log "${GREEN}âœ“${NC} All shell scripts valid"

# Create quick reference card
cat > "$HOME/ARK95X_QUICK_START.txt" << 'EOQUICK'
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                           â•‘
â•‘            ðŸ”¥ ARK95X QUICK START GUIDE ðŸ”¥                â•‘
â•‘                                                           â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ðŸ“ INSTALLATION COMPLETE!

ðŸš€ START SYSTEM:
   ~/ark95x-complete/scripts/start_ark95x.sh

ðŸ¥ HEALTH CHECK:
   python3 ~/ark95x-production/backend/health_monitor.py

ðŸ¤– TEST AI:
   python3 ~/ark95x-complete/scripts/example_usage.py

ðŸ’¬ QUICK CHAT:
   ollama run llama3.2:3b

ðŸ“š FULL DOCUMENTATION:
   cat ~/ark95x-complete/README.md

ðŸ›‘ STOP SYSTEM:
   ~/ark95x-complete/scripts/stop_ark95x.sh

ðŸ“¦ BACKUP:
   ~/ark95x-complete/scripts/backup_ark95x.sh

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

INSTALLED MODELS:
  â€¢ llama3.2:3b      - Fast general purpose
  â€¢ deepseek-r1:7b   - Advanced reasoning
  â€¢ qwen2.5-coder:7b - Code specialist

CONFIGURED APIs:
  â€¢ OpenAI GPT-4
  â€¢ Anthropic Claude
  â€¢ Grok xAI
  â€¢ Perplexity AI

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
EOQUICK

# Display final summary
clear
log ""
log "${GREEN}â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—${NC}"
log "${GREEN}â•‘                                                           â•‘${NC}"
log "${GREEN}â•‘         âœ…  DEPLOYMENT COMPLETE - 100%  âœ…               â•‘${NC}"
log "${GREEN}â•‘                                                           â•‘${NC}"
log "${GREEN}â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${NC}"
log ""
log "${CYAN}ðŸ“¦ COMPONENTS INSTALLED:${NC}"
log "${WHITE}  âœ“ Ollama Local AI${NC}"
log "${WHITE}  âœ“ 3 AI Models (llama3.2, deepseek-r1, qwen2.5-coder)${NC}"
log "${WHITE}  âœ“ API Key Management System${NC}"
log "${WHITE}  âœ“ Unified AI Gateway${NC}"
log "${WHITE}  âœ“ Health Monitoring System${NC}"
log "${WHITE}  âœ“ Auto-start Scripts${NC}"
log "${WHITE}  âœ“ Backup System${NC}"
log "${WHITE}  âœ“ Complete Documentation${NC}"
log ""
log "${CYAN}ðŸ“ QUICK START:${NC}"
log "${YELLOW}  ~/ark95x-complete/scripts/start_ark95x.sh${NC}"
log ""
log "${CYAN}ðŸ“– QUICK REFERENCE:${NC}"
log "${YELLOW}  cat ~/ARK95X_QUICK_START.txt${NC}"
log ""
log "${CYAN}ðŸ“‚ LOCATIONS:${NC}"
log "  Base: ${YELLOW}$BASE_DIR${NC}"
log "  Production: ${YELLOW}$PROD_DIR${NC}"
log "  Logs: ${YELLOW}$LOG_DIR${NC}"
log ""
log "${CYAN}â±ï¸  DEPLOYMENT TIME:${NC}"
DEPLOYMENT_END=$(date +%s)
if [ -n "$DEPLOYMENT_START" ]; then
    DURATION=$((DEPLOYMENT_END - DEPLOYMENT_START))
    log "${WHITE}  $((DURATION / 60)) minutes $((DURATION % 60)) seconds${NC}"
fi
log ""
log "${CYAN}ðŸ“‹ FULL LOG:${NC}"
log "${WHITE}  $LOG_FILE${NC}"
log ""
log "${GREEN}ðŸŽ‰ ARK95X is ready to use! Start with:${NC}"
log "${YELLOW}   ~/ark95x-complete/scripts/start_ark95x.sh${NC}"
log ""

# Set deployment start time at the beginning
DEPLOYMENT_START=$(date +%s)
